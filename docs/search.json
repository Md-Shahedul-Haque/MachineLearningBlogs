[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is a part of my graduate course on Machine Learning (CS 5805).\nArticles are on :\n\nProbabilistic Theory and Random Variable\nClustering\nLinear and Nonlinear Regression\nClassification\nAnomaly / Outlier Detection"
  },
  {
    "objectID": "posts/regression/index.html",
    "href": "posts/regression/index.html",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Supervised machine learning is a type of machine learning where the algorithm learns from labelled data. Labeled data means the dataset whose respective target value is already known. Depending on the output of models, we can divide Supervised machine learning models into two categories:\n\nRegression: It predicts the continuous output variables based on the independent input variable. Such as, the prediction of house prices based on different parameters like house age, distance from the main road, location, area, etc.\nClassification: It predicts the class/category of a datapoint based on the independent input variable. Here, outcome is a categorical or discrete value. For example, given image of an animal is a cat or dog.\n\nRegression computes the relationship between the dependent variables or criterion variables and one or more independent variables or predictors.  \nSeveral types of regression techniques are available. Each of them are suited for different types of data and different types of relationships. The main types of regression techniques are:\n\nLinear Regression\nPolynomial Regression\nStepwise Regression\nDecision Tree Regression\nRandom Forest Regression\nSupport Vector Regression\nRidge Regression\nLasso Regression\nElasticNet Regression\nBayesian Linear Regression"
  },
  {
    "objectID": "posts/regression/index.html#sec-listTypes",
    "href": "posts/regression/index.html#sec-listTypes",
    "title": "Linear and Nonlinear Regression",
    "section": "",
    "text": "Supervised machine learning is a type of machine learning where the algorithm learns from labelled data. Labeled data means the dataset whose respective target value is already known. Depending on the output of models, we can divide Supervised machine learning models into two categories:\n\nRegression: It predicts the continuous output variables based on the independent input variable. Such as, the prediction of house prices based on different parameters like house age, distance from the main road, location, area, etc.\nClassification: It predicts the class/category of a datapoint based on the independent input variable. Here, outcome is a categorical or discrete value. For example, given image of an animal is a cat or dog.\n\nRegression computes the relationship between the dependent variables or criterion variables and one or more independent variables or predictors.  \nSeveral types of regression techniques are available. Each of them are suited for different types of data and different types of relationships. The main types of regression techniques are:\n\nLinear Regression\nPolynomial Regression\nStepwise Regression\nDecision Tree Regression\nRandom Forest Regression\nSupport Vector Regression\nRidge Regression\nLasso Regression\nElasticNet Regression\nBayesian Linear Regression"
  },
  {
    "objectID": "posts/regression/index.html#linear-regression",
    "href": "posts/regression/index.html#linear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "2 Linear Regression",
    "text": "2 Linear Regression\nLinear regression is used for predictive analysis. It is a linear approach for modeling the relationship between the criterion or the scalar response and the multiple predictors or explanatory variables. Linear regression focuses on the conditional probability distribution of the response given the values of the predictors.\n\n2.1 Formula for Linear Regression Model\nThe simplest form of linear regression involved only one independent variable and one dependent variable. The equation for simple linear regression is:\n\\[\ny=\\theta x+b\n\\]\nwhere,\n\n\\(y\\) is the dependent variable\n\\(x\\) is the independent variable\n\\(\\theta\\) is the model weights or parameters\n\\(b\\) is the bias\n\nMultiple Linear Regression involves more than one independent variable and one dependent variable. The equation for multiple linear regression is:\n\\[\ny=b+\\theta_1 x_1+\\theta_2 x_2+\\theta_3 x_3+...+\\theta_n x_n\n\\]\nwhere,\n\n\\(y\\) is the dependent variable\n\\(x_1, x_2, x_3, ..., x_n\\) are the independent variables\n\\(\\theta_1, \\theta_2, ..., \\theta_n\\) are the model weights or parameters\n\\(b\\) is the bias\n\n\n\n2.2 Assumption for Linear Regression Model\nA linear regression model needs to meet a few conditions in order to be accurate and dependable solutions:\n\nLinearity: Linear relation between independent and dependent variable\nIndependence: Observations in the dataset are independent of each other.\nHomoscedasticity: Amount of the independent variable(s) has no impact on the variance of the errors\nNormality: The residuals should be normally distributed\nNo Multicollinearity: No high correlation between the independent variables.\n\n\n\n2.3 California Housing dataset\nWe will be using California Housing dataset for visualizing regression models. This dataset was derived from the 1990 U.S. census, using one row per census block group. Code snippet for loading this dataset and checking its description:\n\n\nimport matplotlib.pylab as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n\n# Load the California Housing dataset\ncalifornia_housing = datasets.fetch_california_housing(as_frame=True)\n\nprint(california_housing.DESCR)\n\n# Load the California Housing dataset as a DataFrame\ndf_california_housing = california_housing.frame\n\n# Display the first few entries of the DataFrame\nprint(df_california_housing.head())\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n\n   Longitude  MedHouseVal  \n0    -122.23        4.526  \n1    -122.22        3.585  \n2    -122.24        3.521  \n3    -122.25        3.413  \n4    -122.25        3.422  \n\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nNext, we choose some features as independent features such as, median income in block group, median house age, average number of rooms and bedrooms per household, average number of household members and block group population. In this case, our target (dependent) variable is median value of the house (in units of \\(100,000\\)). Also we split the \\(20640\\) records for train and test following \\(80-20\\) ratio:\n\n\n# Select the specified features\nfeatures_of_interest = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"AveOccup\", \"Population\"]\n\n# Acquiring independent and dependent variables\nX = df_california_housing[features_of_interest]\ny = df_california_housing['MedHouseVal']\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\n\n\n2.4 Visualization for Linear Regression Model\nFor a simple linear regression model, we utilize the scikit-learn machine library through these three. generic steps:\n\nCreating the desired model\nTrain the model on processed training set\nMaking predictions on the test set and evaluating it\n\nCode snippet for a linear regression model:\n\n\n# Create a linear regression model\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\n\n# Train the model on the training set\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nWe create a generic function which takes prediction label and actual label for evaluating the performance of our model. Code snippet for this function:\n\n\ndef performance_stat(y_test, y_pred):\n  from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n  # Calculate regression metrics\n  mae = mean_absolute_error(y_test, y_pred)\n  mse = mean_squared_error(y_test, y_pred)\n  rmse = np.sqrt(mse)\n  r2 = r2_score(y_test, y_pred)\n\n  print(f'Mean Absolute Error (MAE): {mae:.2f}')\n  print(f'Mean Squared Error (MSE): {mse:.2f}')\n  print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n  print(f'R-squared (R²): {r2:.2f}')\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nLater, we utilize this function to evaluate the performance of our linear regression model:\n\n\nperformance_stat(y_test, y_pred)\n\nMean Absolute Error (MAE): 0.58\nMean Squared Error (MSE): 0.64\nRoot Mean Squared Error (RMSE): 0.80\nR-squared (R²): 0.51\n\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nAn \\(R^2\\) of \\(0.51\\) means that the model explains about \\(51\\%\\) of the variance in the target variable. Since various factors influence house prices, an \\(R^2\\) of \\(0.51\\) indicates a moderate level of explanatory power. The model captures a significant portion of the variability in the target variable.\nNext, we plot predictions by this model with respect to the actual data:\n\n\nplt.figure(figsize=(8, 8))  # Set a square figure size\n# Plot y_pred in red\nplt.scatter(y_test, y_pred, color='red', label='Predicted', alpha=0.1)\n\n# Plot y_test as a line for reference\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='green', linewidth=2, label='Actual')\n\nplt.xlabel('Actual Median House Value')\nplt.ylabel('Predicted Median House Value')\nplt.title('Linear Regression on California Housing Dataset')\nplt.legend()  # Show legend to distinguish between actual and predicted\nplt.show()\n\n\n\n\nFigure 1: Linear Regression on the California Housing dataset\n\n\n\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nSince the red dots (predicted data) is close to the green (actual data) line, this model can predict the target variable well."
  },
  {
    "objectID": "posts/regression/index.html#nonlinear-regression",
    "href": "posts/regression/index.html#nonlinear-regression",
    "title": "Linear and Nonlinear Regression",
    "section": "3 Nonlinear Regression",
    "text": "3 Nonlinear Regression\nNonlinear regression is a statistical technique which assists in describing the non linearity in relationship between independent and dependent variables. Nonlinear regression models are described with a nonlinear equation. Typically, nonlinear regression is well suited to explain relations between variables in real life data. There are a lot of nonlinear regression models, refer to the previously mentioned list of regression techniques in section Section 1 if needed. In the scope of this blog, we confine our discussions to two types of nonlinear regression.\n\n3.1 Support Vector Regressor (SVR)\nSupport vector regression (SVR) is a type of support vector machine (SVM) that is used for regression tasks. It attempts to find a function that best predicts the continuous output value of target for given input values of independent variables. SVR can have both linear and non-linear kernels. We will be using a radial basis function (RBF) kernel which is a non linear one.\nCode snippet for scaling the values of feature so that features with higher values can not dominate the lower value ones. Feature scaling is a must for SVR to speed up convergence and improve model performance. Also we scale the target values accordingly:\n\n\nfrom sklearn.preprocessing import StandardScaler\n# Standardize the features\nscaler_X = StandardScaler()\nX_train_scaled = scaler_X.fit_transform(X_train)\nX_test_scaled = scaler_X.transform(X_test)\n\n# Standardize the target variable\nscaler_y = StandardScaler()\n# Reshape the target variable to a 1D array\ny_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nSimilar to our prior example, we create the regression model at first:\n\n\n# Create an SVR model on RBF kernel\nfrom sklearn.svm import SVR\nmodel = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n\n# Train the model\nmodel.fit(X_train_scaled, y_train_scaled.ravel())\n\n# Make predictions on the test set\ny_pred_scaled = model.predict(X_test_scaled)\n\n# Inverse transform the predictions to the original scale\ny_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nThen, we utilize the predefined function to evaluate the performance of this nonlinear regression model:\n\n\nperformance_stat(y_test, y_pred)\n\nMean Absolute Error (MAE): 0.46\nMean Squared Error (MSE): 0.45\nRoot Mean Squared Error (RMSE): 0.67\nR-squared (R²): 0.66\n\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nHere, we can see \\(R^2\\) of \\(0.66\\) which means that the model explains about \\(66\\%\\) of the variance in the target variable. So this model performs better than the previous model.\nNext, we plot predictions by this model with respect to the actual data:\n\n\nplt.figure(figsize=(8, 8))  # Set a square figure size\n# Plot y_pred in red\nplt.scatter(y_test, y_pred, color='red', label='Predicted', alpha=0.1)\n\n# Plot y_test as a line for reference\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='green', linewidth=2, label='Actual')\n\nplt.xlabel('Actual Median House Value')\nplt.ylabel('Predicted Median House Value')\nplt.title('SVR (RBF Kernel) on California Housing Dataset')\n\nplt.legend()\nplt.show()\n\n\n\n\nFigure 2: Noninear Regression (SVR with RBF Kernel) on the California Housing dataset\n\n\n\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nSince the red dots (predicted data) is closer to the green (actual data) line, this model can predict the target variable well.\n\n\n3.2 Random Forest Regressor\nRandom Forest Regression is a versatile technique for predicting numerical values. To reduce overfitting and improve overall accuracy, It combines the predictions of multiple decision trees.\nSimilar to our previous example, we create the regression model at first:\n\n\n# Create a Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nThen, we use the predefined performance function to evaluate the this nonlinear regression model:\n\n\nperformance_stat(y_test, y_pred)\n\nMean Absolute Error (MAE): 0.46\nMean Squared Error (MSE): 0.42\nRoot Mean Squared Error (RMSE): 0.65\nR-squared (R²): 0.68\n\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nHere, we can see \\(R^2\\) of \\(0.68\\) which means that the model explains about \\(0.68\\%\\) of the variance in the target variable. So this model performs better than the previous models.\nNext, we plot predictions by this model with respect to the actual data:\n\n\nplt.figure(figsize=(8, 8))  # Set a square figure size\n# Plot y_pred in red\nplt.scatter(y_test, y_pred, color='red', label='Predicted', alpha=0.1)\n\n# Plot y_test as a line for reference\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], linestyle='--', color='green', linewidth=2, label='Actual')\n\nplt.xlabel('Actual Median House Value')\nplt.ylabel('Predicted Median House Value')\nplt.title('Random Forest Regressor on California Housing Dataset')\n\nplt.legend()\nplt.show()\n\n\n\n\nFigure 3: Noninear Regression (Random Forest Regressor) on the California Housing dataset\n\n\n\n\nSource: Linear and Nonlinear Regression with scikit-learn Library\nSince the red dots (predicted data) is closer to the green (actual data) line, this model can predict the target variable very well."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Supervised machine learning is a type of machine learning where the algorithm learns from labelled data. Labeled data means the dataset whose respective target value is already known. Depending on the output of models, we can divide Supervised machine learning models into two categories:\n\nRegression: It predicts the continuous output variables based on the independent input variable. Such as, the prediction of house prices based on different parameters like house age, distance from the main road, location, area, etc.\nClassification: It predicts the class/category of a datapoint based on the independent input variable. Here, outcome is a categorical or discrete value. For example, given image of an animal is a cat or dog.\n\nClassification is a method for categorizing data or objects into predefined classes or categories based on their features or attributes. Even though classification comprises a small part of Machine Learning as a whole, it is one of the most important ones."
  },
  {
    "objectID": "posts/classification/index.html#sec-listTypes",
    "href": "posts/classification/index.html#sec-listTypes",
    "title": "Classification",
    "section": "",
    "text": "Supervised machine learning is a type of machine learning where the algorithm learns from labelled data. Labeled data means the dataset whose respective target value is already known. Depending on the output of models, we can divide Supervised machine learning models into two categories:\n\nRegression: It predicts the continuous output variables based on the independent input variable. Such as, the prediction of house prices based on different parameters like house age, distance from the main road, location, area, etc.\nClassification: It predicts the class/category of a datapoint based on the independent input variable. Here, outcome is a categorical or discrete value. For example, given image of an animal is a cat or dog.\n\nClassification is a method for categorizing data or objects into predefined classes or categories based on their features or attributes. Even though classification comprises a small part of Machine Learning as a whole, it is one of the most important ones."
  },
  {
    "objectID": "posts/classification/index.html#types-of-classification",
    "href": "posts/classification/index.html#types-of-classification",
    "title": "Classification",
    "section": "2 Types of Classification",
    "text": "2 Types of Classification\nClassification algorithms are of two types:  \n\nBinary Classification: Classifies the input into one of two classes. Example – on the basis of the given medical image of a person, the model determine whether the person has a certain disease or not.\nMulticlass Classification: Classifies the input into one of several classes or categories. For instance, given an image the model categorize which type of dress it is.\n\n\nDepending on the nature of relationship (linear or nonlinear) between independent and dependent variables, classification can be considered two types:\n\nLinear Classifications: Logistic Regression, Support Vector Machine (with kernel = ‘linear’), Single-layer Perceptron, Stochastic Gradient Descent (SGD) Classifier\nNon-linear Classifications: K-Nearest Neighbours, Support Vector Machine, Naive Bayes, Decision Tree Classifier, Ensemble learning Classifier (Random Forests, Adaboost, Bagging Classifier, Voting Classifier, ExtraTrees Classifier), Multi-layer Artificial Neural Networks\n\nOn the basis of learners, classification can be divided into two types:\n\nLazy Learners: They are instance-based learners since they do not learn a model during the training phase. Instead, they simply store the training data and use it to classify new instances at prediction time. As a result, It is very fast at prediction time because it does not require computations during the predictions. On the other hand, it is less effective in high-dimensional spaces or when the number of training instances is large. Examples include k-nearest neighbors and case-based reasoning.\nEager Learners:They are model-based learners as they learn a model from the training data during the training phase and use this model to classify new instances at prediction time. Consequently, it is slower at prediction time but it is more effective in high-dimensional spaces having large training datasets. Examples of eager learners include decision trees, random forests, and support vector machines."
  },
  {
    "objectID": "posts/classification/index.html#evaluation-metrics",
    "href": "posts/classification/index.html#evaluation-metrics",
    "title": "Classification",
    "section": "3 Evaluation Metrics",
    "text": "3 Evaluation Metrics\nDepending on the specific problem and requirements, we have to choose among several metrics and techniques to evaluate a classification model. Some commonly used evaluation metrics are:\n\nAccuracy: This simple and intuitive metric can be misleading in imbalanced datasets where the majority class dominates the accuracy score.\n\\[\nAccuracy = \\frac{\\mbox{Correctly classified instances}}{\\mbox{Total instances}}\n\\]\nConfusion matrix: A table that shows the number of true positives, true negatives, false positives, and false negatives for each class.\nPrecision and Recall: These metrics are useful in scenarios where there is a trade-off between false positives and false negatives, or when one class is more important than the other.\n\\[\nPrecision = \\frac{\\mbox{True positives}}{\\mbox{Total number of predicted positives}}\n\\]\n\\[\nRecall = \\frac{\\mbox{True positives}}{\\mbox{Total number of actual positives}}\n\\]\nF1-Score: The harmonic mean of precision and recall, calculated as\n\\[\nF1-Score = \\frac{2 *\\mbox{precision} * \\mbox{recall}}{\\mbox{precision + recall}}\n\\]\nIt is also a useful metric for imbalanced datasets where both precision and recall are important.\nROC curve and AUC: The Receiver Operating Characteristic (ROC) curve is a plot of the true positive rate (recall) against the false positive rate (1-specificity) for different threshold values of the classifier's decision function. The Area Under the Curve (AUC) measures the overall performance of the classifier, with values ranging from 0.5 (random guessing) to 1 (perfect classification).\nCross-validation: A technique for dividing the available data into multiple folds and training the model on each fold while testing on the others, to obtain a more robust estimate of the model's performance."
  },
  {
    "objectID": "posts/classification/index.html#visualization",
    "href": "posts/classification/index.html#visualization",
    "title": "Classification",
    "section": "4 Visualization",
    "text": "4 Visualization\nTo understand how classification models works, we study three classifiers and compare their performance a standard data set, the Iris data set. \n\n4.1 Iris Dataset\nWe will be using Iris Dataset for visualizing classifier models. This dataset was first used by Sir R.A. Fisher in his paper. This is perhaps the best known database to be found in the pattern recognition literature. The data set contains three classes of \\(50\\) instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two; the latter are not linearly separable from each other. Features in this dataset are sepal length, sepal width, petal length, petal width in cm. Target variable can be of these three categories: Setosa, Versicolour, Virginica (represented by \\(0, 1, 2\\) respectively in the dataset).\nCode snippet for loading this dataset and checking its description:\n\n\n# Importing the required libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n\n# import the iris dataset\niris = datasets.load_iris()\nprint(iris.DESCR)\nX = iris.data\ny = iris.target\n\n# splitting X and y into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\n.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher's paper. Note that it's the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher's paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n     Mathematical Statistics\" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n\n\nSource: Classification with scikit-learn Library\nAlso we split the \\(50\\) records for train and test following \\(80-20\\) ratio\n\n\n4.2 Support Vector Machine\nSupport Vector Machine (SVM) is mostly used for linear or nonlinear classification. But its application is also observed in regression problems, and even outlier detection tasks. SVM algorithms are very effective as they try to find the maximum separating hyperplane between the different classes available in the target feature.\nWe utilize the scikit-learn machine library to create our SVM model with linear kernel. Before training we perform feature scaling which is a must for SVM to speed up convergence and improve model performance. Code snippet for this is:\n\n\nfrom sklearn.preprocessing import StandardScaler\n# Standardize the features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Create an SVM model on linear kernel\nfrom sklearn import svm\nsvm_clf = svm.SVC(kernel='linear')  # Linear Kernel\n# train the model\nsvm_clf.fit(X_train_scaled, y_train)\n# make predictions\nsvm_clf_pred = svm_clf.predict(X_test_scaled)\n\n\nSource: Classification with scikit-learn Library\n\n\n4.3 Naive Bayes Classifier\nNaive Bayes classifiers are a collection of classification algorithms based on Bayes' Theorem. Such classifiers has strict assumption that every pair of features is independent of each other. Also they require that each features provide an independent and equal contribution to the outcome\nCode snippet for creating a gaussian naive bayes classifier model:\n\n\nfrom sklearn.naive_bayes import GaussianNB\n# Create an NB model\ngnb = GaussianNB()\n# train the model\ngnb.fit(X_train, y_train)\n# make predictions\ngnb_pred = gnb.predict(X_test)\n\nSource: Classification with scikit-learn Library\n\n\n4.4 Decision Tree Classifier\nDecision tree is a versatile supervised learning algorithms used for both classification and regression tasks. It consists of a flowchart-like tree structure. Here, each internal node denotes a test on an attribute, each branch represents an outcome of the test. Each leaf node (terminal node) holds a class label. During training, it is constructed by recursively splitting the training data into subsets based on the values of the attributes until a stopping criterion is met, such as the maximum depth of the tree or the minimum number of samples required to split a node.\nCode snippet for creating a decision tree classifier model:\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Create an DTC model\ndt = DecisionTreeClassifier(random_state=42)\n# train the model\ndt.fit(X_train, y_train)\n# make predictions\ndt_pred = dt.predict(X_test)\n\nSource: Classification with scikit-learn Library\n\n\n4.5 Performance Evaluation\nLater, we evaluate the performance of the models:\n\n\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Evaluate the models\nsvm_accuracy = accuracy_score(y_test, svm_clf_pred)\ndt_accuracy = accuracy_score(y_test, dt_pred)\ngnb_accuracy = accuracy_score(y_test, gnb_pred)\n\n# Create a DataFrame for classification reports\nclassification_reports = pd.DataFrame(columns=['accuracy', 'precision', 'recall', 'f1-score'])\n\n# Classification Report for SVM\nsvm_classification_report = classification_report(y_test, svm_clf_pred, output_dict=True)\nsvm_metrics = [svm_accuracy, svm_classification_report['0']['precision'], svm_classification_report['0']['recall'], svm_classification_report['0']['f1-score']]\nclassification_reports.loc['Support Vector Machine'] = svm_metrics\n\n# Classification Report for Naive Bayes\nnb_classification_report = classification_report(y_test, gnb_pred, output_dict=True)\nnb_metrics = [gnb_accuracy, nb_classification_report['0']['precision'], nb_classification_report['0']['recall'], nb_classification_report['0']['f1-score']]\nclassification_reports.loc['Naive Bayes'] = nb_metrics\n\n# Classification Report for Decision Tree\ntree_classification_report = classification_report(y_test, dt_pred, output_dict=True)\ntree_metrics = [dt_accuracy, tree_classification_report['0']['precision'], tree_classification_report['0']['recall'], tree_classification_report['0']['f1-score']]\nclassification_reports.loc['Decision Tree'] = tree_metrics\n\n# Display the DataFrame\nprint(classification_reports)\n\n               accuracy  precision  recall  f1-score\nSVM            0.966667        1.0     1.0       1.0\nDecision Tree  1.000000        1.0     1.0       1.0\nNaive Bayes    1.000000        1.0     1.0       1.0\n\n\nSource: Classification with scikit-learn Library\nSince, the dataset is too small, performance of the models are pretty close except Support Vector Machine. To understand better, we take a closer at the predictions made by the models though confusion matrix:\n\n\nfrom sklearn.metrics import confusion_matrix\n\n# Confusion Matrix for SVM\nsvm_conf_matrix = confusion_matrix(y_test, svm_clf_pred)\n\n# Confusion Matrix for Decision Tree\ndt_conf_matrix = confusion_matrix(y_test, dt_pred)\n\n# Confusion Matrix for Naive Bayes\ngnb_conf_matrix = confusion_matrix(y_test, gnb_pred)\n\n# Plot Confusion Matrices\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nlabels = iris.target_names\n\n# Create subplots\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n\n# SVM Confusion Matrix\nsns.heatmap(svm_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=axes[0])\naxes[0].set_title('Confusion Matrix for Support Vector Machine')\n\n# Naive Bayes Confusion Matrix\nsns.heatmap(gnb_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=axes[1])\naxes[1].set_title('Confusion Matrix for Gaussian Naive Bayes')\n\n# Decision Tree Confusion Matrix\nsns.heatmap(dt_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=axes[2])\naxes[2].set_title('Confusion Matrix for Decision Tree')\n\n# Add common x-axis label\nfig.text(0.5, 0.01, 'Predicted', ha='center', va='center', fontsize=14)\n\n# Add common y-axis label\nfig.text(0.02, 0.5, 'Actual', ha='center', va='center', rotation='vertical', fontsize=14)\n\n# Adjust layout\nplt.tight_layout()\n\n\n# Show the plot\nplt.show()\n\n\n\n\nSource: Classification with scikit-learn Library\nAs we can predict, Decision Tree and Naive Bayes perform the best. But Support Vector Machine makes a mistake while predicting a sample “veriscolor” class (mistook it as “virginica” class)."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Broadly there are three types of learning methods in ML:\n\nSupervised learning method: Label for corresponding datapoint is available.\nUnsupervised learning method: Label for corresponding datapoint is unavailable.\nSemisupervised learning method: Lable is available only for a small portion of all the datapoints, most of the datapoints lack label.\n\nClustering belongs to the category of unsupervised learning method. Generally, it is utilized as a process for finding meaningful structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples. Clustering is the task of dividing the population or data points into a number of disjoint sets such that data points in the same sets are more similar to other data points in the same set and dissimilar to the data points in other sets. Consequently, clustering forms a collection of objects on the basis of similarity and dissimilarity between them."
  },
  {
    "objectID": "posts/clustering/index.html#clustering-in-machine-learning",
    "href": "posts/clustering/index.html#clustering-in-machine-learning",
    "title": "Clustering",
    "section": "",
    "text": "Broadly there are three types of learning methods in ML:\n\nSupervised learning method: Label for corresponding datapoint is available.\nUnsupervised learning method: Label for corresponding datapoint is unavailable.\nSemisupervised learning method: Lable is available only for a small portion of all the datapoints, most of the datapoints lack label.\n\nClustering belongs to the category of unsupervised learning method. Generally, it is utilized as a process for finding meaningful structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples. Clustering is the task of dividing the population or data points into a number of disjoint sets such that data points in the same sets are more similar to other data points in the same set and dissimilar to the data points in other sets. Consequently, clustering forms a collection of objects on the basis of similarity and dissimilarity between them."
  },
  {
    "objectID": "posts/clustering/index.html#clustering-methods",
    "href": "posts/clustering/index.html#clustering-methods",
    "title": "Clustering",
    "section": "2 Clustering methods",
    "text": "2 Clustering methods\nClustering shapes the intrinsic grouping among the unlabelled data. There are no standard for good clustering. It depends on the user, and what criteria they may use which satisfy their need. Based on the assumptions and goal of the task, clustering algorithms can be divided into several categories: \n\nDensity-Based Methods: Dense region has some similarities and is different from the lower dense region. For example, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), OPTICS (Ordering Points to Identify Clustering Structure), etc.\nHierarchical Based Methods: Forms a tree-type structure based on the hierarchy. Later, new clusters are formed using the previously formed one. It is further divided into two categories:\n\nAgglomerative (bottom-up approach)\nDivisive (top-down approach)\nExamples include CURE (Clustering Using Representatives), BIRCH (Balanced Iterative Reducing Clustering and using Hierarchies), etc.\n\nPartitioning Methods: Partitions the datapoints into \\(k\\) clusters and each partition forms one cluster. Focus is optimizing an objective criterion similarity function. For instance, K-means, CLARANS (Clustering Large Applications based upon Randomized Search), etc.\nGrid-based Methods: Data space is formulated into a finite number of cells that form. Such as, STING (Statistical Information Grid), WaveCluster, CLIQUE (CLustering In Quest), etc."
  },
  {
    "objectID": "posts/clustering/index.html#visualization",
    "href": "posts/clustering/index.html#visualization",
    "title": "Clustering",
    "section": "3 Visualization",
    "text": "3 Visualization\n\n3.1 K-means Clustering\nIt is the simplest clustering algorithm. It partitions given \\(n\\) observations into predefined \\(k\\) clusters. The basis for such clustering is the nearest mean from the \\(k\\) centroids. Clusters formed in K-Means are spherical or convex in shape.\nCode snippet for K-means clustering on the Palmer Penguins dataset:\n\n\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\npenguins = pd.read_csv(\"https://pos.it/palmer-penguins-github-csv\")\n\n# Display the first few rows of the dataset\nprint(penguins.head())\n\n# Drop missing values\npenguins = penguins.dropna()\n\n# Select relevant features for clustering (e.g., flipper length and body mass)\nfeatures = penguins[['flipper_length_mm', 'body_mass_g']]\n\n\n# Perform K-means clustering with 3 clusters\nnum_clusters = 3\nkmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\npenguins.loc[:, 'Cluster'] = kmeans.fit_predict(features)\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  year  \n0       3750.0    male  2007  \n1       3800.0  female  2007  \n2       3250.0  female  2007  \n3          NaN     NaN  2007  \n4       3450.0  female  2007  \n\n\nSource: Clustering with scikit-learn Library\nIf the user choose number of clusters to be \\(3\\) and relevant features for clustering are Flipper Length (mm) and Body Mass (g), we can observe the following clustering:\n\n\n\n\n\n\nFigure 1: K-means clustering on the Palmer Penguins dataset\n\n\n\n\nSource: Clustering with scikit-learn Library\nWe can observe the centroid (marked as red circle) of the three clusters from K-means clustering.\n\n\n3.2 DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nPartitioning methods (K-means, PAM clustering) and hierarchical clustering focus on finding spherical-shaped clusters or convex clusters. As a result, they are suitable only for compact and well-separated clusters. Moreover, their performance deteriorate with the presence of noise and outliers in the data. DBSCAN can handle dataset containing non-convex shape clusters and outliers. The basis for this clustering are the maximum distance between two samples for them to be considered neighbors and the minimum number of samples in a neighborhood within that radius.\nCode snippet for DBSCAN on the Palmer Penguins dataset:\n\n\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\n\npenguins = pd.read_csv(\"https://pos.it/palmer-penguins-github-csv\")\n\n# Drop missing values\npenguins = penguins.dropna()\n\n# Select relevant features for clustering (e.g., flipper length and body mass)\nfeatures = penguins[['flipper_length_mm', 'body_mass_g']]\n\n\n# Perform DBSCAN clustering\n# Adjust eps and min_samples based on data\ndbscan = DBSCAN(eps=75, min_samples=15)\npenguins.loc[:, 'Cluster'] = dbscan.fit_predict(features)\n\nSource: Clustering with scikit-learn Library\nIf the user choose the maximum distance between two samples to be \\(75\\), the minimum number of samples in a neighborhood within that radius to be \\(15\\) and relevant features for clustering are Flipper Length (mm) and Body Mass (g), we can observe the following clustering:\n\n\n\n\n\n\nFigure 2: DBSCAN on the Palmer Penguins dataset\n\n\n\n\nSource: Clustering with scikit-learn Library\nEven though it got us three clusters, this one is quite different from the previous one we got from K-means clustering. This clustering provides irregular shape other than the spherical-shaped clusters or convex clusters."
  },
  {
    "objectID": "posts/randomVariable/index.html",
    "href": "posts/randomVariable/index.html",
    "title": "Random Variable",
    "section": "",
    "text": "A random variable (stochastic variable) is a real valued function from the domain of the sample space of a defined experiment. Generally, a random variable is denoted by capital letter (usually \\(X\\) ) where small letter (\\(x\\)) denotes the observed value. For example, \\(X\\) be the measurement of number of heads observed in an experiment of tossing two coins. Here, the sample space \\(S\\) is:\n\\[ S = {(H, H), (H, T), (T, H), (T, T) } \\]\nThis 4 possible outcomes of this experiment constitute the domain of $X$. For each outcome, the random variable is calculates in this way:\n\\[  X(H, H) = 2  \\]$$ X(H, T) = 1 $$\n\\[  X(T, H) = 1 \\] $$ X(T, T) = 0 $$"
  },
  {
    "objectID": "posts/randomVariable/index.html#probability-theory-and-random-variable-in-machine-learning",
    "href": "posts/randomVariable/index.html#probability-theory-and-random-variable-in-machine-learning",
    "title": "Random Variable",
    "section": "",
    "text": "A random variable (stochastic variable) is a real valued function from the domain of the sample space of a defined experiment. Generally, a random variable is denoted by capital letter (usually \\(X\\) ) where small letter (\\(x\\)) denotes the observed value. For example, \\(X\\) be the measurement of number of heads observed in an experiment of tossing two coins. Here, the sample space \\(S\\) is:\n\\[ S = {(H, H), (H, T), (T, H), (T, T) } \\]\nThis 4 possible outcomes of this experiment constitute the domain of $X$. For each outcome, the random variable is calculates in this way:\n\\[  X(H, H) = 2  \\]$$ X(H, T) = 1 $$\n\\[  X(T, H) = 1 \\] $$ X(T, T) = 0 $$"
  },
  {
    "objectID": "posts/randomVariable/index.html#types-of-random-variable",
    "href": "posts/randomVariable/index.html#types-of-random-variable",
    "title": "Random Variable",
    "section": "2 Types of Random Variable",
    "text": "2 Types of Random Variable\nIn broader way, there are two categories of random variable:\n\nDiscrete Random Variable: Range of the observable value is a finite set. For instance, in the previous two coins tossing \\(X\\) is a discrete random variable with observable values \\(\\{0, 1, 2\\}\\) .\nContinuous Random Variable: Range of the observable value has some interval, bounded or unbounded. For instance, weight of different people in a class."
  },
  {
    "objectID": "posts/randomVariable/index.html#density-functions",
    "href": "posts/randomVariable/index.html#density-functions",
    "title": "Random Variable",
    "section": "3 Density Functions",
    "text": "3 Density Functions\n\n3.1 Probability Mass Function (PMF)\nThe probability mass function (PMF) a discrete random variable is the likelihood that the variable takes on a given value. Mathematically,\n\\[  f(x) = P(X = x) \\]\nwhich follows the following properties:\n\\[  f(x) \\geq 0 \\mbox{ for any $x$ \\(\\epsilon\\)\n$S$ } \\]\n\n\\[  \\sum_{x \\epsilon S} f(x) = 1  \\]\nCode snippet for calculating PMF of a discrete random variable which follows binomial distribution:\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import binom\n# parameters for the binomial distribution\nn = 10\np = 0.5\n\n# generate the values for x\nx = np.arange(0, n+1)\n\n# calculate the PMF\npmf = binom.pmf(x, n, p)\n\nSource: Random Variable with Scipy Library\nThen we can plot the PMF of that variable better visualization\n\n\n\n\n\n\nFigure 1: Probability Mass Function (PMF) of a discrete random variable with binomial distribution\n\n\n\n\nSource: Random Variable with Scipy Library\n\n\n3.2 Probability Density Function (PDF)\nThe probability density function (PDF) of a continuous random variable is the likelihood that the variable lies on a given range. Mathematically,\n\\[ \\int_{a}^{b} f(x)dx = P(a&lt; X &lt;b) \\mbox{ for any $a, b$ \\(\\epsilon\\) $S$ satisfying a &lt; b }\\]\nwhich follows the following properties:\n\\[  f(x) \\geq 0 \\mbox{ for any $x$ \\(\\epsilon\\)\n$S$ } \\]\n\\[  \\int_{x \\epsilon S} f(x)dx = 1  \\]\nCode snippet for calculating PDF of a continuous random variable which follows normal distribution:\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# parameters for the normal distribution\nmu = 0\nsigma = 1\n\n# generate the values for x\nx = np.linspace(-10, 10, 1000)\n# calculate the PDF\npdf = norm.pdf(x, mu, sigma)\n\nSource: Random Variable with Scipy Library\nThen we can plot the PDF of that variable better visualization\n\n\n\n\n\n\nFigure 2: Probability Density Function (PDF) of a continous random variable with normal distribution\n\n\n\n\nSource: Random Variable with Scipy Library\n\n\n3.3 Cumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) of a random variable is the likelihood that the variable is bounded within a given value. Mathematically,\n\\[ F(x) = \\sum_{z \\epsilon S, z \\leq x } f(z) \\mbox{ for discrete random variables }\\]\n\\[ F(x) = \\int_{- \\infty}^{x} f(z)dz  \\mbox{ for continuous random variables }\\]\nFurthermore,\n\\[  P(a&lt; X \\leq b) = F(b) - F(a)  \\]\nCode snippet for calculating CDF of a continuous random variable which follows normal distribution:\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# parameters for the normal distribution\nmu = 0\nsigma = 1\n\n# generate the values for x\nx = np.linspace(-10, 10, 1000)\n# calculate the PDF\ncdf = norm.cdf(x, mu, sigma)\n\nSource: Random Variable with Scipy Library\nThen we can plot the CDF of that variable better visualization\n\n\n\n\n\n\nFigure 3: Cumulative Distribution Function (CDF) of a continous random variable with normal distribution\n\n\n\n\nSource: Random Variable with Scipy Library"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MachineLearningBlogs",
    "section": "",
    "text": "Classification\n\n\n\n\n\n\n\nClassification\n\n\nCode\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nMd Shahedul Haque\n\n\n\n\n\n\n  \n\n\n\n\nLinear and Nonlinear Regression\n\n\n\n\n\n\n\nLinear Regression\n\n\nNonlinear Regression\n\n\nCode\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nMd Shahedul Haque\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nClustering\n\n\nCode\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nMd Shahedul Haque\n\n\n\n\n\n\n  \n\n\n\n\nRandom Variable\n\n\n\n\n\n\n\nProbability Theory\n\n\nRandom Variable\n\n\nCode\n\n\nVisualization\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nMd Shahedul Haque\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nFirst Blog\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\nMd Shahedul Haque (Shawon)\n\n\n\n\n\n\nNo matching items"
  }
]