[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/randomVariable/index.html",
    "href": "posts/randomVariable/index.html",
    "title": "Random Variable",
    "section": "",
    "text": "A random variable (stochastic variable) is a real valued function from the domain of the sample space of a defined experiment. Generally, a random variable is denoted by capital letter (usually \\(X\\) ) where small letter (\\(x\\)) denotes the observed value. For example, \\(X\\) be the measurement of number of heads observed in an experiment of tossing two coins. Here, the sample space \\(S\\) is:\n\\[ S = {(H, H), (H, T), (T, H), (T, T) } \\]\nThis 4 possible outcomes of this experiment constitute the domain of $X$. For each outcome, the random variable is calculates in this way:\n\\[  X(H, H) = 2  \\]$$ X(H, T) = 1 $$\n\\[  X(T, H) = 1 \\] $$ X(T, T) = 0 $$"
  },
  {
    "objectID": "posts/randomVariable/index.html#random-variable-in-machine-learning",
    "href": "posts/randomVariable/index.html#random-variable-in-machine-learning",
    "title": "Random Variable",
    "section": "",
    "text": "A random variable (stochastic variable) is a real valued function from the domain of the sample space of a defined experiment. Generally, a random variable is denoted by capital letter (usually \\(X\\) ) where small letter (\\(x\\)) denotes the observed value. For example, \\(X\\) be the measurement of number of heads observed in an experiment of tossing two coins. Here, the sample space \\(S\\) is:\n\\[ S = {(H, H), (H, T), (T, H), (T, T) } \\]\nThis 4 possible outcomes of this experiment constitute the domain of $X$. For each outcome, the random variable is calculates in this way:\n\\[  X(H, H) = 2  \\]$$ X(H, T) = 1 $$\n\\[  X(T, H) = 1 \\] $$ X(T, T) = 0 $$"
  },
  {
    "objectID": "posts/randomVariable/index.html#types-of-random-variable",
    "href": "posts/randomVariable/index.html#types-of-random-variable",
    "title": "Random Variable",
    "section": "Types of Random Variable",
    "text": "Types of Random Variable\nIn broader way, there are two categories of random variable:\n\nDiscrete Random Variable: Range of the observable value is a finite set. For instance, in the previous two coins tossing \\(X\\) is a discrete random variable with observable values \\(\\{0, 1, 2\\}\\) .\nContinuous Random Variable: Range of the observable value has some interval, bounded or unbounded. For instance, weight of different people in a class."
  },
  {
    "objectID": "posts/randomVariable/index.html#density-functions",
    "href": "posts/randomVariable/index.html#density-functions",
    "title": "Random Variable",
    "section": "Density Functions",
    "text": "Density Functions\n\nProbability Mass Function (PMF)\nThe probability mass function (PMF) a discrete random variable is the likelihood that the variable takes on a given value. Mathematically,\n\\[  f(x) = P(X = x) \\]\nwhich follows the following properties:\n\\[  f(x) \\geq 0 \\mbox{ for any $x$ \\(\\epsilon\\)\n$S$ } \\]\n\n\\[  \\sum_{x \\epsilon S} f(x) = 1  \\]\nCode snippet for calculating PMF of a discrete random variable which follows binomial distribution:\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import binom\n# parameters for the binomial distribution\nn = 10\np = 0.5\n\n# generate the values for x\nx = np.arange(0, n+1)\n\n# calculate the PMF\npmf = binom.pmf(x, n, p)\n\nSource: Random Variable with Scipy Library\nThen we can plot the PMF of that variable better visualization\n\n\n\n\n\n\nFigure 1: Probability Mass Function (PMF) of a discrete random variable with binomial distribution\n\n\n\n\nSource: Random Variable with Scipy Library\n\n\nProbability Density Function (PDF)\nThe probability density function (PDF) of a continuous random variable is the likelihood that the variable lies on a given range. Mathematically,\n\\[ \\int_{a}^{b} f(x)dx = P(a&lt; X &lt;b) \\mbox{ for any $a, b$ \\(\\epsilon\\) $S$ satisfying a &lt; b }\\]\nwhich follows the following properties:\n\\[  f(x) \\geq 0 \\mbox{ for any $x$ \\(\\epsilon\\)\n$S$ } \\]\n\\[  \\int_{x \\epsilon S} f(x)dx = 1  \\]\nCode snippet for calculating PDF of a continuous random variable which follows normal distribution:\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# parameters for the normal distribution\nmu = 0\nsigma = 1\n\n# generate the values for x\nx = np.linspace(-10, 10, 1000)\n# calculate the PDF\npdf = norm.pdf(x, mu, sigma)\n\nSource: Random Variable with Scipy Library\nThen we can plot the PDF of that variable better visualization\n\n\n\n\n\n\nFigure 2: Probability Density Function (PDF) of a continous random variable with normal distribution\n\n\n\n\nSource: Random Variable with Scipy Library\n\n\nCumulative Distribution Function (CDF)\nThe cumulative distribution function (CDF) of a random variable is the likelihood that the variable is bounded within a given value. Mathematically,\n\\[ F(x) = \\sum_{z \\epsilon S, z \\leq x } f(z) \\mbox{ for discrete random variables }\\]\n\\[ F(x) = \\int_{- \\infty}^{x} f(z)dz  \\mbox{ for continuous random variables }\\]\nFurthermore,\n\\[  P(a&lt; X \\leq b) = F(b) - F(a)  \\]\nCode snippet for calculating CDF of a continuous random variable which follows normal distribution:\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm\n\n# parameters for the normal distribution\nmu = 0\nsigma = 1\n\n# generate the values for x\nx = np.linspace(-10, 10, 1000)\n# calculate the PDF\ncdf = norm.cdf(x, mu, sigma)\n\nSource: Random Variable with Scipy Library\nThen we can plot the CDF of that variable better visualization\n\n\n\n\n\n\nFigure 3: Cumulative Distribution Function (CDF) of a continous random variable with normal distribution\n\n\n\n\nSource: Random Variable with Scipy Library"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Broadly there are three types of learning methods in ML:\n\nSupervised learning method: Label for corresponding datapoint is available.\nUnsupervised learning method: Label for corresponding datapoint is unavailable.\nSemisupervised learning method: Lable is available only for a small portion of all the datapoints, most of the datapoints lack label.\n\nClustering belongs to the category of unsupervised learning method. Generally, it is utilized as a process for finding meaningful structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples. Clustering is the task of dividing the population or data points into a number of disjoint sets such that data points in the same sets are more similar to other data points in the same set and dissimilar to the data points in other sets. Consequently, clustering forms a collection of objects on the basis of similarity and dissimilarity between them."
  },
  {
    "objectID": "posts/clustering/index.html#clustering-in-machine-learning",
    "href": "posts/clustering/index.html#clustering-in-machine-learning",
    "title": "Clustering",
    "section": "",
    "text": "Broadly there are three types of learning methods in ML:\n\nSupervised learning method: Label for corresponding datapoint is available.\nUnsupervised learning method: Label for corresponding datapoint is unavailable.\nSemisupervised learning method: Lable is available only for a small portion of all the datapoints, most of the datapoints lack label.\n\nClustering belongs to the category of unsupervised learning method. Generally, it is utilized as a process for finding meaningful structure, explanatory underlying processes, generative features, and groupings inherent in a set of examples. Clustering is the task of dividing the population or data points into a number of disjoint sets such that data points in the same sets are more similar to other data points in the same set and dissimilar to the data points in other sets. Consequently, clustering forms a collection of objects on the basis of similarity and dissimilarity between them."
  },
  {
    "objectID": "posts/clustering/index.html#clustering-methods",
    "href": "posts/clustering/index.html#clustering-methods",
    "title": "Clustering",
    "section": "Clustering methods",
    "text": "Clustering methods\nClustering shapes the intrinsic grouping among the unlabelled data. There are no standard for good clustering. It depends on the user, and what criteria they may use which satisfy their need. Based on the assumptions and goal of the task, clustering algorithms can be divided into several categories: \n\nDensity-Based Methods: Dense region has some similarities and is different from the lower dense region. For example, DBSCAN (Density-Based Spatial Clustering of Applications with Noise), OPTICS (Ordering Points to Identify Clustering Structure), etc.\nHierarchical Based Methods: Forms a tree-type structure based on the hierarchy. Later, new clusters are formed using the previously formed one. It is further divided into two categories:\n\nAgglomerative (bottom-up approach)\nDivisive (top-down approach)\nExamples include CURE (Clustering Using Representatives), BIRCH (Balanced Iterative Reducing Clustering and using Hierarchies), etc.\n\nPartitioning Methods: Partitions the datapoints into \\(k\\) clusters and each partition forms one cluster. Focus is optimizing an objective criterion similarity function. For instance, K-means, CLARANS (Clustering Large Applications based upon Randomized Search), etc.\nGrid-based Methods: Data space is formulated into a finite number of cells that form. Such as, STING (Statistical Information Grid), WaveCluster, CLIQUE (CLustering In Quest), etc."
  },
  {
    "objectID": "posts/clustering/index.html#density-functions-visualization",
    "href": "posts/clustering/index.html#density-functions-visualization",
    "title": "Clustering",
    "section": "Density Functions Visualization",
    "text": "Density Functions Visualization\n\nK-means Clustering\nIt is the simplest clustering algorithm. It partitions given \\(n\\) observations into predefined \\(k\\) clusters. The basis for such clustering is the nearest mean from the \\(k\\) centroids. Clusters formed in K-Means are spherical or convex in shape.\nCode snippet for K-means clustering on the Palmer Penguins dataset:\n\n\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\npenguins = pd.read_csv(\"https://pos.it/palmer-penguins-github-csv\")\n\n# Display the first few rows of the dataset\nprint(penguins.head())\n\n# Drop missing values\npenguins = penguins.dropna()\n\n# Select relevant features for clustering (e.g., flipper length and body mass)\nfeatures = penguins[['flipper_length_mm', 'body_mass_g']]\n\n\n# Perform K-means clustering with 3 clusters\nnum_clusters = 3\nkmeans = KMeans(n_clusters=num_clusters, n_init=10, random_state=42)\npenguins.loc[:, 'Cluster'] = kmeans.fit_predict(features)\n\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n0  Adelie  Torgersen            39.1           18.7              181.0   \n1  Adelie  Torgersen            39.5           17.4              186.0   \n2  Adelie  Torgersen            40.3           18.0              195.0   \n3  Adelie  Torgersen             NaN            NaN                NaN   \n4  Adelie  Torgersen            36.7           19.3              193.0   \n\n   body_mass_g     sex  year  \n0       3750.0    male  2007  \n1       3800.0  female  2007  \n2       3250.0  female  2007  \n3          NaN     NaN  2007  \n4       3450.0  female  2007  \n\n\nSource: Clustering with scikit-learn Library\nIf the user choose number of clusters to be \\(3\\) and relevant features for clustering are Flipper Length (mm) and Body Mass (g), we can observe the following clustering:\n\n\n\n\n\n\nFigure 1: K-means clustering on the Palmer Penguins dataset\n\n\n\n\nSource: Clustering with scikit-learn Library\nWe can observe the centroid (marked as red circle) of the three clusters from K-means clustering.\n\n\n\n\n\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise)\nPartitioning methods (K-means, PAM clustering) and hierarchical clustering focus on finding spherical-shaped clusters or convex clusters. As a result, they are suitable only for compact and well-separated clusters. Moreover, their performance deteriorate with the presence of noise and outliers in the data. DBSCAN can handle dataset containing non-convex shape clusters and outliers. The basis for this clustering are the maximum distance between two samples for them to be considered neighbors and the minimum number of samples in a neighborhood within that radius.\nCode snippet for DBSCAN on the Palmer Penguins dataset:\n\n\nimport pandas as pd\nfrom sklearn.cluster import DBSCAN\nimport matplotlib.pyplot as plt\n\npenguins = pd.read_csv(\"https://pos.it/palmer-penguins-github-csv\")\n\n# Drop missing values\npenguins = penguins.dropna()\n\n# Select relevant features for clustering (e.g., flipper length and body mass)\nfeatures = penguins[['flipper_length_mm', 'body_mass_g']]\n\n\n# Perform DBSCAN clustering\n# Adjust eps and min_samples based on data\ndbscan = DBSCAN(eps=75, min_samples=15)\npenguins.loc[:, 'Cluster'] = dbscan.fit_predict(features)\n\nSource: Clustering with scikit-learn Library\nIf the user choose the maximum distance between two samples to be \\(75\\), the minimum number of samples in a neighborhood within that radius to be \\(15\\) and relevant features for clustering are Flipper Length (mm) and Body Mass (g), we can observe the following clustering:\n\n\n\n\n\n\nFigure 2: DBSCAN on the Palmer Penguins dataset\n\n\n\n\nSource: Clustering with scikit-learn Library\nEven though it got us three clusters, this one is quite different from the previous one we got from K-means clustering. This clustering provides irregular shape other than the spherical-shaped clusters or convex clusters."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is my first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MachineLearningBlogs",
    "section": "",
    "text": "Clustering\n\n\n\n\n\n\n\nclustering\n\n\ncode\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2023\n\n\nMd Shahedul Haque\n\n\n\n\n\n\n  \n\n\n\n\nRandom Variable\n\n\n\n\n\n\n\nrandom variable\n\n\ncode\n\n\nvisualization\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nMd Shahedul Haque\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2023\n\n\nMD S H\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nStart\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2023\n\n\nMD S H\n\n\n\n\n\n\nNo matching items"
  }
]